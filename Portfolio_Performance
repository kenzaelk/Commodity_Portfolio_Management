{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10092419,"sourceType":"datasetVersion","datasetId":6223494}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"PPO vs TD3","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.decomposition import PCA\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nimport torch\nimport torch.nn as nn\nimport gym\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\n# Adaptive Normalization\nwindow_size = 126  \nrolling_means = returns.rolling(window=window_size, min_periods=1).mean()\nrolling_stds = returns.rolling(window=window_size, min_periods=1).std()\nrolling_stds.replace(0, np.nan, inplace=True)\nadaptive_returns = (returns - rolling_means) / rolling_stds\nadaptive_returns.dropna(inplace=True)\n\n# Ensure 'returns' and 'adaptive_returns' have matching indexes\naligned_returns = returns.loc[adaptive_returns.index].copy()\n\n# Apply PCA to reduced adaptive returns\npca = PCA(n_components=4)\nreduced_features = pca.fit_transform(adaptive_returns)\n\n# K-Means Clustering\nn_clusters = 4  \nkmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\naligned_returns['Cluster'] = kmeans.fit_predict(reduced_features)\n\n# One-Hot Encode Clusters\nohe = OneHotEncoder(sparse=False)\ncluster_features = ohe.fit_transform(aligned_returns[['Cluster']])\ncluster_features_df = pd.DataFrame(cluster_features, columns=[f'Cluster_{i}' for i in range(n_clusters)], index=aligned_returns.index)\n\n# Merge into final dataset\nreturns = pd.concat([aligned_returns.drop(columns=['Cluster']), cluster_features_df], axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import LSTM, Dense, Input, Dropout\nfrom tensorflow.keras.optimizers import Adam\nimport gym\nfrom stable_baselines3 import PPO, TD3\nfrom stable_baselines3.common.vec_env import DummyVecEnv\n\n# Custom Portfolio Environment\nclass PortfolioEnv(gym.Env):\n    def __init__(self, returns):\n        super(PortfolioEnv, self).__init__()\n        self.returns = returns\n        self.n_assets = returns.shape[1] - (n_clusters + 1)  # Excluding cluster labels and one-hot features\n        self.observation_dim = self.n_assets + n_clusters  # Ensure correct shape\n        self.action_space = gym.spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)\n        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(self.observation_dim,), dtype=np.float32)\n        self.current_step = 0\n    \n    def reset(self):\n        self.current_step = 0\n        obs = np.append(\n            self.returns.iloc[self.current_step, :-n_clusters-1].values,\n            self.returns.iloc[self.current_step, -n_clusters:].values\n        )\n        return obs[:self.observation_dim]  \n    \n    def step(self, action):\n        self.current_step += 1\n        done = self.current_step >= len(self.returns) - 1\n        \n        portfolio_return = np.dot(action, self.returns.iloc[self.current_step, :-n_clusters-1].values)\n        volatility = np.std(np.dot(self.returns.iloc[:self.current_step+1, :-n_clusters-1], action))\n        var_threshold = np.percentile(portfolio_return, 5)\n        cvar = np.mean(portfolio_return[portfolio_return <= var_threshold])\n        r2r = portfolio_return / (volatility + 1e-6)\n        \n        reward =  abs(cvar) + r2r\n        obs = np.append(\n            self.returns.iloc[self.current_step, :-n_clusters-1].values,\n            self.returns.iloc[self.current_step, -n_clusters:].values\n        )\n        \n        return obs[:self.observation_dim], reward, done, {\"r2r\": r2r, \"cvar\": cvar, \"portfolio_return\": portfolio_return}\n    \n# Create Environment\nenv = DummyVecEnv([lambda: PortfolioEnv(returns)])\n\n# Train PPO Model\nppo_model = PPO(\"MlpPolicy\", env, verbose=1)\nppo_model.learn(total_timesteps=100000)\n\n# Train TD3 Model\ntd3_model = TD3(\"MlpPolicy\", env, verbose=1)\ntd3_model.learn(total_timesteps=100000)\n\n# Evaluate Performance with clipped rewards\ndef evaluate_model(model, env, print_interval=100, reward_clip_range=(-1, 1)):\n    obs = env.reset()\n    done, rewards, cumulative_returns = False, [], []\n    total_return = 1\n    metrics = {\"r2r\": [], \"cvar\": [], \"portfolio_return\": []}\n    \n    step_counter = 0\n    \n    while not done:\n        action, _ = model.predict(obs)\n        obs, reward, done, info = env.step(action)\n        \n        # Clip rewards to avoid overflow\n        reward = np.clip(reward, reward_clip_range[0], reward_clip_range[1])\n        \n        rewards.append(reward)\n        total_return *= (1 + reward)\n        \n        # Prevent total_return from reaching infinity\n        if np.isnan(total_return) or np.isinf(total_return):\n            total_return = np.clip(total_return, -1e10, 1e10)\n        \n        cumulative_returns.append(total_return)\n        \n        info = info[0]  \n        metrics[\"r2r\"].append(info[\"r2r\"])\n        metrics[\"cvar\"].append(info[\"cvar\"])\n        metrics[\"portfolio_return\"].append(info[\"portfolio_return\"])\n\n        #step_counter += 1\n        \n    return np.mean(rewards), cumulative_returns, metrics\n\nppo_reward, ppo_cum_returns, ppo_metrics = evaluate_model(ppo_model, env)\ntd3_reward, td3_cum_returns, td3_metrics = evaluate_model(td3_model, env)\n\nprint(f\"PPO Metrics - R2R: {np.mean(ppo_metrics['r2r'])}, CVaR: {np.mean(ppo_metrics['cvar'])}\")\nprint(f\"TD3 Metrics -  R2R: {np.mean(td3_metrics['r2r'])}, CVaR: {np.mean(td3_metrics['cvar'])}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert portfolio returns to Pandas Series\nppo_portfolio_return = pd.Series(ppo_metrics[\"portfolio_return\"])\ntd3_portfolio_return = pd.Series(td3_metrics[\"portfolio_return\"])\n\n# Compute rolling volatility (standard deviation over a 6-month window)\nppo_rolling_vol = ppo_portfolio_return.rolling(window=126).std()\ntd3_rolling_vol = td3_portfolio_return.rolling(window=126).std()\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nplt.plot(dates, ppo_rolling_vol, label=\"PPO Rolling Volatility\", color=\"blue\")\nplt.plot(dates, td3_rolling_vol, label=\"TD3 Rolling Volatility\", color=\"red\")\n\nplt.xlabel(\"Date\")\nplt.ylabel(\"Volatility (Rolling 6 Months)\")\nplt.title(\"Rolling Volatility Over Time (6-Month Window)\")\nplt.legend()\nplt.grid(True)\nplt.xticks(rotation=45)\nplt.show()\n\n\n# Summarize portfolio return contributions\nppo_total_return = sum(ppo_metrics[\"portfolio_return\"])\ntd3_total_return = sum(td3_metrics[\"portfolio_return\"])\n\n# Pie chart labels\nlabels = [\"PPO Portfolio Return\", \"TD3 Portfolio Return\"]\nreturns = [ppo_total_return, td3_total_return]\n\n# Define colors\ncolors = [\"blue\", \"red\"]\n\n# Create Pie Chart\nplt.figure(figsize=(8, 6))\nplt.pie(returns, labels=labels, autopct=\"%1.1f%%\", colors=colors, startangle=140, explode=[0.05, 0])\nplt.title(\"Portfolio Return Contribution (PPO vs TD3)\")\nplt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"PPO-LSTM","metadata":{}},{"cell_type":"code","source":"# === Custom Portfolio Environment ===\nclass PortfolioEnv(gym.Env):\n    def __init__(self, returns, n_clusters):\n        super(PortfolioEnv, self).__init__()\n        self.returns = returns\n        self.n_assets = len(price_cols)\n        self.n_clusters = n_clusters\n        self.action_space = gym.spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)\n        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_assets + self.n_clusters,), dtype=np.float32)\n        self.current_step = 0\n        self.initial_capital = 1.0\n\n    def reset(self):\n        self.current_step = 0\n        self.capital = self.initial_capital\n        obs = np.append(self.returns.iloc[self.current_step, :-self.n_clusters].values, \n                        self.returns.iloc[self.current_step, -self.n_clusters:].values)\n        obs = self._fix_observation_shape(obs)\n        return np.nan_to_num(obs)\n\n    def step(self, action):\n        self.current_step += 1\n        done = self.current_step >= len(self.returns) - 1\n\n        action = np.nan_to_num(action)\n        exp_a = np.exp(action - np.max(action))  \n        action = exp_a / np.sum(exp_a)\n\n        portfolio_return = np.dot(action, self.returns.iloc[self.current_step, :-self.n_clusters].values)\n        self.capital *= (1 + portfolio_return)\n\n        rolling_window = min(126, self.current_step + 1)\n        volatility = np.std(np.dot(self.returns.iloc[self.current_step - rolling_window + 1:self.current_step + 1, :-self.n_clusters], action))\n        volatility = max(volatility, 1e-6)\n\n        past_returns = np.dot(self.returns.iloc[:self.current_step + 1, :-self.n_clusters], action)\n        var_threshold = np.percentile(past_returns, 5)\n        cvar = np.mean(past_returns[past_returns <= var_threshold]) if np.any(past_returns <= var_threshold) else 0\n\n        alpha, beta = 0.3, 0.2\n        r2r = portfolio_return / volatility\n        drawdown_penalty = abs(min(0, portfolio_return))\n        reward = r2r - alpha * abs(cvar) - beta * drawdown_penalty\n\n        obs = np.append(self.returns.iloc[self.current_step, :-self.n_clusters].values,\n                        self.returns.iloc[self.current_step, -self.n_clusters:].values)\n        obs = self._fix_observation_shape(obs)\n        return np.nan_to_num(obs), reward, done, {\"capital\": self.capital, \"r2r\": r2r, \"cvar\": cvar}\n\n    def _fix_observation_shape(self, obs):\n        expected_shape = self.n_assets + self.n_clusters\n        if obs.shape[0] > expected_shape:\n            obs = obs[:expected_shape]\n        elif obs.shape[0] < expected_shape:\n            obs = np.pad(obs, (0, expected_shape - obs.shape[0]), 'constant')\n        return obs\n\n# === LSTM Feature Extractor ===\nclass LSTMFeatureExtractor(BaseFeaturesExtractor):\n    def __init__(self, observation_space, features_dim=128):\n        super(LSTMFeatureExtractor, self).__init__(observation_space, features_dim)\n        input_dim = observation_space.shape[0]\n        self.lstm = nn.LSTM(input_dim, features_dim, batch_first=True)\n        self.flatten = nn.Flatten()\n\n    def forward(self, observations):\n        observations = observations.unsqueeze(1)\n        lstm_out, _ = self.lstm(observations)\n        lstm_out = self.flatten(lstm_out[:, -1, :])\n        return lstm_out\n\n# === Evaluation Function ===\ndef evaluate_model(model, env, threshold=0.0):  \n    obs = env.reset()\n    done, cumulative_returns = False, []\n    r2r_values, cvar_values = [], []\n    daily_returns = []\n\n    while not done:\n        action, _ = model.predict(np.nan_to_num(obs))\n        obs, reward, done, info = env.step(action)\n        cumulative_returns.append(info[0][\"capital\"])\n        r2r_values.append(info[0][\"r2r\"])\n        cvar_values.append(info[0][\"cvar\"])\n\n        if len(cumulative_returns) > 1:\n            ret = cumulative_returns[-1] / cumulative_returns[-2] - 1\n            daily_returns.append(ret)\n\n    peak = np.maximum.accumulate(cumulative_returns)\n    drawdowns = (peak - cumulative_returns) / peak\n    max_drawdown = np.max(drawdowns)\n\n    gains = [r for r in daily_returns if r > threshold]\n    losses = [abs(r) for r in daily_returns if r < threshold]\n    omega_ratio = (np.sum(gains) / np.sum(losses)) if np.sum(losses) > 0 else np.inf\n\n    return None, cumulative_returns, r2r_values, cvar_values, max_drawdown, omega_ratio\n\ntrain_returns = returns.copy()\ntest_returns = returns.copy()\n\n# === Multi-Seed Training ===\nppo_results = []\nppo_lstm_results = []\nseeds = [0, 42, 123, 777, 999]\n\nfor seed in seeds:\n    print(f\"\\n=== Training with Seed: {seed} ===\")\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    train_env = DummyVecEnv([lambda: PortfolioEnv(train_returns, n_clusters)])\n    test_env = DummyVecEnv([lambda: PortfolioEnv(test_returns, n_clusters)])\n\n    # PPO-LSTM\n    ppo_lstm_model = PPO(\n        \"MlpPolicy\", train_env,\n        policy_kwargs={\"features_extractor_class\": LSTMFeatureExtractor},\n        gamma=0.993, learning_rate=0.0002, batch_size=128, ent_coef=0.012, verbose=0, seed=seed\n    )\n    ppo_lstm_model.learn(total_timesteps=100000)\n\n    _, cum_returns, r2r_vals, cvar_vals, max_dd, omega = evaluate_model(ppo_lstm_model, test_env)\n    ppo_lstm_results.append({\n        \"final_return\": cum_returns[-1],\n        \"r2r\": np.mean(r2r_vals),\n        \"cvar\": np.mean(cvar_vals),\n        \"max_dd\": max_dd,\n        \"omega\": omega,\n        \"capital_curve\": cum_returns,\n        \"drawdown_curve\": (np.maximum.accumulate(cum_returns) - cum_returns) / np.maximum.accumulate(cum_returns)\n    })\n\n    # PPO\n    ppo_model = PPO(\n        \"MlpPolicy\", train_env,\n        gamma=0.91, learning_rate=0.0002, batch_size=64, ent_coef=0.05, verbose=0, seed=seed\n    )\n    ppo_model.learn(total_timesteps=100000)\n\n    _, cum_returns, r2r_vals, cvar_vals, max_dd, omega = evaluate_model(ppo_model, test_env)\n    ppo_results.append({\n        \"final_return\": cum_returns[-1],\n        \"r2r\": np.mean(r2r_vals),\n        \"cvar\": np.mean(cvar_vals),\n        \"max_dd\": max_dd,\n        \"omega\": omega,\n        \"capital_curve\": cum_returns,\n        \"drawdown_curve\": (np.maximum.accumulate(cum_returns) - cum_returns) / np.maximum.accumulate(cum_returns)\n    })\n\n# === Summarize Results ===\ndef summarize_results(results, label):\n    df = pd.DataFrame(results)\n    print(f\"\\n{label} Results (mean ± std):\")\n    print(f\"Final Return: {df['final_return'].mean():.4f} ± {df['final_return'].std():.4f}\")\n    print(f\"R2R: {df['r2r'].mean():.4f} ± {df['r2r'].std():.4f}\")\n    print(f\"CVaR: {df['cvar'].mean():.4f} ± {df['cvar'].std():.4f}\")\n    print(f\"Max Drawdown: {df['max_dd'].mean():.4f} ± {df['max_dd'].std():.4f}\")\n    print(f\"Omega: {df['omega'].mean():.4f} ± {df['omega'].std():.4f}\")\n\nsummarize_results(ppo_lstm_results, \"PPO-LSTM\")\nsummarize_results(ppo_results, \"PPO\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import itertools\n# Calculate average cumulative return across all seeds\nppo_lstm_final_returns = [result['final_return'] for result in ppo_lstm_results]\nppo_final_returns = [result['final_return'] for result in ppo_results]\n\n# Convert to percentages\navg_lstm_return = np.mean(ppo_lstm_final_returns) * 100\navg_ppo_return = np.mean(ppo_final_returns) * 100\n\n# Calculate performance improvement\nimprovement = avg_lstm_return - avg_ppo_return\nrelative_improvement = (improvement / avg_ppo_return) * 100\n\n# Print results\nprint(f\"PPO-LSTM Average Return: {avg_lstm_return:.2f}%\")\nprint(f\"PPO Average Return: {avg_ppo_return:.2f}%\")\nprint(f\"Absolute Improvement: {improvement:.2f}%\")\nprint(f\"Relative Improvement: {relative_improvement:.2f}%\")\n\n\n# Define distinct styles\nppo_colors = plt.cm.Blues(np.linspace(0.4, 1, len(seeds)))\nppo_lstm_colors = plt.cm.Reds(np.linspace(0.4, 1, len(seeds)))\nline_styles = ['-', '--', '-.', ':', (0, (3, 5, 1, 5))]\n\nplt.figure(figsize=(12, 6))\n\n# Plot PPO results\nfor i, result in enumerate(ppo_results):\n    dates_for_plot = data.index[-len(result['capital_curve']):]\n    plt.plot(dates_for_plot,\n             result['capital_curve'],\n             color=ppo_colors[i],\n             linestyle=line_styles[i % len(line_styles)],\n             linewidth=2,\n             label=f'PPO Seed {seeds[i]}')\n\n# Plot PPO-LSTM results\nfor i, result in enumerate(ppo_lstm_results):\n    dates_for_plot = data.index[-len(result['capital_curve']):]\n    plt.plot(dates_for_plot,\n             result['capital_curve'],\n             color=ppo_lstm_colors[i],\n             linestyle=line_styles[i % len(line_styles)],\n             linewidth=2,\n             label=f'PPO-LSTM Seed {seeds[i]}')\n\nplt.title('Capital Growth Comparison (PPO vs PPO-LSTM)')\nplt.xlabel('Date')\nplt.ylabel('Capital')\nplt.legend(loc='upper left', fontsize=9)\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()\n\n\n# Define consistent colors\nppo_color = 'steelblue'\nppo_lstm_color = 'firebrick'\n\n# Create the figure\nplt.figure(figsize=(12, 6))\n\n# PPO Drawdown (solid lines)\nfor i, result in enumerate(ppo_results):\n    dates_for_plot = data.index[-len(result['drawdown_curve']):]\n    plt.plot(dates_for_plot,\n             result['drawdown_curve'],\n             label=f'PPO Seed {seeds[i]}',\n             color=ppo_color,\n             linestyle='-',\n             linewidth=1.8)\n\n# PPO-LSTM Drawdown (dashed lines)\nfor i, result in enumerate(ppo_lstm_results):\n    dates_for_plot = data.index[-len(result['drawdown_curve']):]\n    plt.plot(dates_for_plot,\n             result['drawdown_curve'],\n             label=f'PPO-LSTM Seed {seeds[i]}',\n             color=ppo_lstm_color,\n             linestyle='--',\n             linewidth=2.2)\n\n# Final formatting\nplt.title('Drawdown Comparison (PPO vs PPO-LSTM)', fontsize=14)\nplt.xlabel('Date')\nplt.ylabel('Drawdown')\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.legend(fontsize=10, loc='upper left')\nplt.tight_layout()\nplt.show()\n\n\nfinal_returns_ppolstm = [result['final_return'] for result in ppo_lstm_results]\nfinal_returns_ppo = [result['final_return'] for result in ppo_results]\nplt.figure(figsize=(8, 6))\nplt.boxplot([final_returns_ppolstm, final_returns_ppo], labels=['PPO-LSTM', 'PPO'])\nplt.title('Final Return Comparison (Boxplot)')\nplt.ylabel('Final Return')\nplt.grid(True)\nplt.show()\n\n\n\nwindow = 126  # ~6 months\n\ncapital_curve = np.array(ppo_lstm_results[0]['capital_curve'])\ndaily_returns = np.diff(capital_curve) / capital_curve[:-1]\nreturns_series = pd.Series(daily_returns)\nrolling_vol = returns_series.rolling(window).std() * np.sqrt(252)\nrolling_ret = pd.Series(capital_curve).pct_change(window)\n\nvol_dates = data.index[-len(rolling_vol):]\nret_dates = data.index[-len(rolling_ret):]\n\nfig, ax1 = plt.subplots(figsize=(12, 5))\n\nax1.set_xlabel('Date')\nax1.set_ylabel('Volatility', color='blue')\nax1.plot(vol_dates, rolling_vol, color='blue', label='Rolling Volatility')\nax1.tick_params(axis='y', labelcolor='blue')\n\nax2 = ax1.twinx()\nax2.set_ylabel('Return', color='red')\nax2.plot(ret_dates, rolling_ret, color='red', linestyle='--', label='Rolling Return')\nax2.tick_params(axis='y', labelcolor='red')\n\nplt.title('6-Month Rolling Volatility and Return')\nfig.tight_layout()\nplt.grid(True)\nplt.show()\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}