{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10092419,"sourceType":"datasetVersion","datasetId":6223494}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\n\n# Step 1: Load & Preprocess Data\ndata = pd.read_csv('commodities_DAILY.csv')\ndata['Dates'] = pd.to_datetime(data['Dates'])\ndata = data.sort_values(by='Dates')\n\n# Select price columns\nprice_cols = [col for col in data.columns if col.endswith('_PX_LAST')]\n\n# Compute log returns\nreturns = np.log(data[price_cols] / data[price_cols].shift(1))\nreturns.dropna(inplace=True)\n\n# Step 2: Compute Features Per Commodity\ncommodity_features = []\nfor commodity in price_cols:\n    series = returns[commodity].dropna()\n    mean_return = series.mean()\n    volatility = series.std()\n    skewness = series.skew()\n    commodity_features.append([mean_return, volatility, skewness])\n\n# Create DataFrame where rows are commodities and columns are features\ncommodity_df = pd.DataFrame(commodity_features, columns=[\"Mean Return\", \"Volatility\", \"Skewness\"], index=price_cols)\n\n# Standardize features\nscaler = StandardScaler()\nnormalized_features = scaler.fit_transform(commodity_df)\n\n# Step 3: K-Means Clustering\nn_clusters = 4  \nkmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\ncommodity_df['Cluster_KMeans'] = kmeans.fit_predict(normalized_features)\n\n# Step 4: Deep K-Means using Autoencoder\ndef build_autoencoder(input_dim):\n    input_layer = Input(shape=(input_dim,))\n    encoded = Dense(64, activation='relu')(input_layer)\n    encoded = Dense(32, activation='relu')(encoded)\n    encoded = Dense(16, activation='relu')(encoded)\n    decoded = Dense(32, activation='relu')(encoded)\n    decoded = Dense(64, activation='relu')(decoded)\n    decoded = Dense(input_dim, activation='linear')(decoded)\n    \n    autoencoder = Model(input_layer, decoded)\n    encoder = Model(input_layer, encoded)\n    \n    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n    return autoencoder, encoder\n\nautoencoder, encoder = build_autoencoder(normalized_features.shape[1])\n\n# Train Autoencoder\nautoencoder.fit(normalized_features, normalized_features, epochs=50, batch_size=8, verbose=1)\n\n# Extract deep features\ndeep_features = encoder.predict(normalized_features)\n\n# Apply K-Means to deep features\ndeep_kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\ncommodity_df['Cluster_DeepKMeans'] = deep_kmeans.fit_predict(deep_features)\n\n# Step 5: Compute Correlations\n\n# 1. Overall correlation\noverall_corr = returns.corr()\n\n# 2. Within-cluster correlation for K-Means\nwithin_cluster_corr_kmeans = {}\nfor cluster in range(n_clusters):\n    cluster_assets = commodity_df[commodity_df['Cluster_KMeans'] == cluster].index\n    within_cluster_corr_kmeans[cluster] = returns[cluster_assets].corr()\n\n# 3. Within-cluster correlation for Deep K-Means\nwithin_cluster_corr_deep = {}\nfor cluster in range(n_clusters):\n    cluster_assets = commodity_df[commodity_df['Cluster_DeepKMeans'] == cluster].index\n    within_cluster_corr_deep[cluster] = returns[cluster_assets].corr()\n\n# 4. Between-cluster correlation for K-Means\ncluster_means_kmeans = returns.groupby(commodity_df['Cluster_KMeans'], axis=1).mean()\nbetween_cluster_corr_kmeans = cluster_means_kmeans.corr()\n\n# 5. Between-cluster correlation for Deep K-Means\ncluster_means_deep = returns.groupby(commodity_df['Cluster_DeepKMeans'], axis=1).mean()\nbetween_cluster_corr_deep = cluster_means_deep.corr()\n\n# Step 6: Visualizing Correlations\n\nplt.figure(figsize=(18, 6))\n\n# Overall correlation heatmap\nplt.subplot(1, 3, 1)\nsns.heatmap(overall_corr, cmap=\"coolwarm\", center=0, annot=False)\nplt.title(\"Overall Correlation\")\n\n# K-Means: Between-cluster correlation\nplt.subplot(1, 3, 2)\nsns.heatmap(between_cluster_corr_kmeans, cmap=\"coolwarm\", center=0, annot=True)\nplt.title(\"Between-Cluster Correlation (K-Means)\")\n\n# Deep K-Means: Between-cluster correlation\nplt.subplot(1, 3, 3)\nsns.heatmap(between_cluster_corr_deep, cmap=\"coolwarm\", center=0, annot=True)\nplt.title(\"Between-Cluster Correlation (Deep K-Means)\")\n\nplt.tight_layout()\nplt.show()\n\n# Show within-cluster correlations for K-Means\nfor cluster, corr_matrix in within_cluster_corr_kmeans.items():\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(corr_matrix, cmap=\"coolwarm\", center=0, annot=True)\n    plt.title(f\"Within-Cluster Correlation (K-Means, Cluster {cluster})\")\n    plt.show()\n\n# Show within-cluster correlations for Deep K-Means\nfor cluster, corr_matrix in within_cluster_corr_deep.items():\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(corr_matrix, cmap=\"coolwarm\", center=0, annot=True)\n    plt.title(f\"Within-Cluster Correlation (Deep K-Means, Cluster {cluster})\")\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import silhouette_score\n\nadaptive_returns = returns.iloc[:, 1:].values  \nscaler = StandardScaler()\nadaptive_returns = scaler.fit_transform(adaptive_returns)\n\ninput_dim = adaptive_returns.shape[1]\nencoding_dim = 5  \n\ninput_layer = Input(shape=(input_dim,))\nencoded = Dense(encoding_dim, activation='relu')(input_layer)\ndecoded = Dense(input_dim, activation='linear')(encoded)\n\nautoencoder = Model(input_layer, decoded)\nautoencoder.compile(optimizer='adam', loss='mse')\nautoencoder.fit(adaptive_returns, adaptive_returns, epochs=50, batch_size=16, verbose=0)\n\nencoder = Model(input_layer, encoded)\nencoded_features = encoder.predict(adaptive_returns)\n\n# Step 2: Determine Optimal Clusters using Elbow Method & Silhouette Score\ndef evaluate_clusters(data, max_clusters=10):\n    wcss = []\n    silhouette_scores = []\n    \n    for k in range(2, max_clusters + 1):\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n        cluster_labels = kmeans.fit_predict(data)\n        wcss.append(kmeans.inertia_)\n        silhouette_scores.append(silhouette_score(data, cluster_labels))\n    \n    return wcss, silhouette_scores\n\nwcss_kmeans, silhouette_kmeans = evaluate_clusters(adaptive_returns)\nwcss_deep, silhouette_deep = evaluate_clusters(encoded_features)\n\n# Plot Elbow Method\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(2, len(wcss_kmeans) + 2), wcss_kmeans, marker='o', linestyle='--', label='K-Means')\nplt.plot(range(2, len(wcss_deep) + 2), wcss_deep, marker='s', linestyle='-', label='Deep K-Means')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.title('Elbow Method for Clustering')\nplt.legend()\n\n# Plot Silhouette Score\nplt.subplot(1, 2, 2)\nplt.plot(range(2, len(silhouette_kmeans) + 2), silhouette_kmeans, marker='o', linestyle='--', label='K-Means')\nplt.plot(range(2, len(silhouette_deep) + 2), silhouette_deep, marker='s', linestyle='-', label='Deep K-Means')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Score for Clustering')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import LSTM, Dense, Input, Dropout\nfrom tensorflow.keras.optimizers import Adam\nimport gym\n\n\nscaler = StandardScaler()\nnormalized_returns = scaler.fit_transform(returns)\nnormalized_returns = pd.DataFrame(normalized_returns, columns=returns.columns, index=returns.index)\n\n# Adaptive Normalization: Rolling-window scaling (6 months â‰ˆ 126 trading days)\nwindow_size = 126\nrolling_means = returns.rolling(window=window_size).mean()\nrolling_stds = returns.rolling(window=window_size).std()\nadaptive_returns = (returns - rolling_means) / rolling_stds\nadaptive_returns.dropna(inplace=True)\n\nadaptive_returns = adaptive_returns.iloc[-len(returns):]\nreturns = returns.iloc[-len(adaptive_returns):].copy()\n\n#n_clusters = 4  \nkmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\nkmeans_labels = kmeans.fit_predict(adaptive_returns)\nreturns['Cluster'] = kmeans_labels\n\ndef build_autoencoder(input_dim):\n    input_layer = Input(shape=(input_dim,))\n    encoded = Dense(128, activation='relu')(input_layer)\n    encoded = Dense(64, activation='relu')(encoded)\n    encoded = Dense(32, activation='relu')(encoded)\n    decoded = Dense(64, activation='relu')(encoded)\n    decoded = Dense(128, activation='relu')(decoded)\n    decoded = Dense(input_dim, activation='linear')(decoded)\n    \n    autoencoder = Model(input_layer, decoded)\n    encoder = Model(input_layer, encoded)\n    \n    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n    return autoencoder, encoder\n\ninput_dim = adaptive_returns.shape[1]\nautoencoder, encoder = build_autoencoder(input_dim)\n\nautoencoder.fit(adaptive_returns, adaptive_returns, epochs=50, batch_size=16, verbose=1)\n\nencoded_features = encoder.predict(adaptive_returns)\n\ndeep_kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\ndeep_cluster_labels = deep_kmeans.fit_predict(encoded_features)\nreturns['Deep_Cluster'] = deep_cluster_labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"LSTM vs. Clustering-Based LSTM","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import LSTM, Dense, Input, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n\n# Step 3: Return Forecasting (Optimized LSTM per Cluster)\ndef build_lstm_model(input_shape, dropout_rate=0.2):\n    model = Sequential([\n        LSTM(64, return_sequences=True, input_shape=input_shape),\n        Dropout(dropout_rate),\n        LSTM(64, return_sequences=False),\n        Dropout(dropout_rate),\n        Dense(1)\n    ])\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n    return model\n\n# Train LSTM on Full Dataset\nfull_X = np.expand_dims(adaptive_returns.values[:-1], axis=1)\nfull_Y = adaptive_returns.values[1:, 0]  # Predicting next step return\nlstm_full = build_lstm_model(full_X.shape[1:])\nlstm_full.fit(full_X, full_Y, epochs=20, batch_size=32, verbose=1)\nfull_pred = lstm_full.predict(full_X)\n\n# Train LSTM for K-Means Clusters\nkmeans_models = {}\nkmeans_preds = {}\nfor cluster in range(n_clusters):\n    cluster_data = returns[returns['Cluster'] == cluster].iloc[:, :-2]  # Exclude cluster labels\n    if len(cluster_data) > 1:  # Ensure enough data for training\n        X = np.expand_dims(cluster_data.values[:-1], axis=1)\n        Y = cluster_data.values[1:, 0]  # Predict next step return\n        model = build_lstm_model(X.shape[1:], dropout_rate=0.3)  # Optimized dropout rate\n        model.fit(X, Y, epochs=20, batch_size=32, verbose=1)\n        kmeans_models[cluster] = model\n        kmeans_preds[cluster] = model.predict(X)\n\n# Train LSTM for Deep K-Means Clusters\ndeep_kmeans_models = {}\ndeep_kmeans_preds = {}\nfor cluster in range(n_clusters):\n    cluster_data = returns[returns['Deep_Cluster'] == cluster].iloc[:, :-2]  # Exclude cluster labels\n    if len(cluster_data) > 1:\n        X = np.expand_dims(cluster_data.values[:-1], axis=1)\n        Y = cluster_data.values[1:, 0]\n        model = build_lstm_model(X.shape[1:], dropout_rate=0.3)\n        model.fit(X, Y, epochs=20, batch_size=32, verbose=1)\n        deep_kmeans_models[cluster] = model\n        deep_kmeans_preds[cluster] = model.predict(X)\n\n# Evaluate Forecasting Performance\ndef reward_to_risk(predicted_values):\n    mean_return = np.mean(predicted_values)\n    risk = np.std(predicted_values)\n    return mean_return / risk if risk != 0 else 0\n\ndef evaluate_forecasting(true_values, predicted_values):\n    mae = mean_absolute_error(true_values, predicted_values)\n    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n    r2r = reward_to_risk(predicted_values)\n    return mae, rmse, r2r\n\nmae_full, rmse_full, r2r_full = evaluate_forecasting(full_Y, full_pred)\nprint(\"LSTM without Clustering - MAE:\", mae_full, \"RMSE:\", rmse_full, \"R2R:\", r2r_full)\n\nfor cluster, model in kmeans_models.items():\n    true_values = returns[returns['Cluster'] == cluster].iloc[1:, 0].values\n    pred_values = kmeans_preds[cluster]\n    mae, rmse, r2r = evaluate_forecasting(true_values, pred_values)\n    print(f\"LSTM for K-Means Cluster {cluster} - MAE: {mae}, RMSE: {rmse}, R2R: {r2r}\")\n\nfor cluster, model in deep_kmeans_models.items():\n    true_values = returns[returns['Deep_Cluster'] == cluster].iloc[1:, 0].values\n    pred_values = deep_kmeans_preds[cluster]\n    mae, rmse, r2r = evaluate_forecasting(true_values, pred_values)\n    print(f\"LSTM for Deep K-Means Cluster {cluster} - MAE: {mae}, RMSE: {rmse}, R2R: {r2r}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install optuna","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Performance Improvements with Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"import optuna\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import LSTM, Dropout, Dense\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\n\n# Define hyperparameter optimization function\ndef objective(trial):\n    # Hyperparameter search space\n    lstm_units = trial.suggest_categorical(\"lstm_units\", [32, 64, 128])\n    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n    epochs = trial.suggest_int(\"epochs\", 20, 50)\n\n    # Define LSTM model\n    model = Sequential([\n        LSTM(lstm_units, return_sequences=True, input_shape=(1, adaptive_returns.shape[1])),\n        Dropout(dropout_rate),\n        LSTM(lstm_units, return_sequences=False),\n        Dropout(dropout_rate),\n        Dense(1)\n    ])\n\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n\n    # Prepare Data\n    X = np.expand_dims(adaptive_returns.values[:-1], axis=1)\n    Y = adaptive_returns.values[1:, 0]\n\n    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n    # Train Model\n    model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=0, validation_data=(X_val, Y_val))\n\n    # Evaluate on validation set\n    val_pred = model.predict(X_val)\n    rmse = np.sqrt(mean_squared_error(Y_val, val_pred))\n\n    return rmse  # Minimize RMSE\n\n# Run Optuna Optimization\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=20)\n\n# Best Hyperparameters\nbest_params = study.best_params\nprint(\"Best Hyperparameters:\", best_params)\n\n# Prepare Data Again (Outside Optuna)\nX = np.expand_dims(adaptive_returns.values[:-1], axis=1)\nY = adaptive_returns.values[1:, 0]\n\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n# **Now Define final_model using best hyperparameters**\nfinal_model = Sequential([\n    LSTM(best_params[\"lstm_units\"], return_sequences=True, input_shape=(1, adaptive_returns.shape[1])),\n    Dropout(best_params[\"dropout_rate\"]),\n    LSTM(best_params[\"lstm_units\"], return_sequences=False),\n    Dropout(best_params[\"dropout_rate\"]),\n    Dense(1)\n])\n\nfinal_model.compile(optimizer=Adam(learning_rate=best_params[\"learning_rate\"]), loss='mse')\n\n# **Train with Best Parameters**\nfinal_model.fit(X_train, Y_train, epochs=best_params[\"epochs\"], batch_size=best_params[\"batch_size\"], verbose=1)\n\n# Predictions\nfinal_pred = final_model.predict(X_train)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import LSTM, Dropout, Dense\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Function to build LSTM model\ndef build_lstm_model(input_shape, lstm_units, dropout_rate, learning_rate):\n    model = Sequential([\n        LSTM(lstm_units, return_sequences=True, input_shape=input_shape),\n        Dropout(dropout_rate),\n        LSTM(lstm_units, return_sequences=False),\n        Dropout(dropout_rate),\n        Dense(1)\n    ])\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n    return model\n\n# Function to optimize LSTM hyperparameters per cluster\ndef optimize_lstm_for_cluster(cluster_data, cluster_label):\n    def objective(trial):\n        # Hyperparameter search space\n        lstm_units = trial.suggest_categorical(\"lstm_units\", [32, 64, 128])\n        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n        learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n        batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n        epochs = trial.suggest_int(\"epochs\", 20, 50)\n\n        # Prepare Data\n        X = np.expand_dims(cluster_data.values[:-1], axis=1)\n        Y = cluster_data.values[1:, 0]\n\n        X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n        # Build & Train Model\n        model = build_lstm_model((1, cluster_data.shape[1]), lstm_units, dropout_rate, learning_rate)\n        model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=0, validation_data=(X_val, Y_val))\n\n        # Evaluate on Validation Set\n        val_pred = model.predict(X_val)\n        rmse = np.sqrt(mean_squared_error(Y_val, val_pred))\n\n        return rmse  # Minimize RMSE\n\n    # Run Optuna Optimization for this cluster\n    print(f\"Optimizing LSTM for Cluster {cluster_label}...\")\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=10)\n\n    print(f\"Best Hyperparameters for Cluster {cluster_label}: {study.best_params}\")\n    return study.best_params\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dictionary to store trained models\noptimized_kmeans_models = {}\noptimized_deep_kmeans_models = {}\n\n# Train LSTM per K-Means Cluster\nfor cluster in range(n_clusters):\n    cluster_data = returns[returns['Cluster'] == cluster].iloc[:, :-2]  # Exclude cluster labels\n    if len(cluster_data) > 1:\n        best_params = optimize_lstm_for_cluster(cluster_data, cluster)\n        \n        # Prepare Data Again\n        X = np.expand_dims(cluster_data.values[:-1], axis=1)\n        Y = cluster_data.values[1:, 0]\n        X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n        # Train Final Optimized Model\n        model = build_lstm_model((1, cluster_data.shape[1]), best_params[\"lstm_units\"], best_params[\"dropout_rate\"], best_params[\"learning_rate\"])\n        model.fit(X_train, Y_train, epochs=best_params[\"epochs\"], batch_size=best_params[\"batch_size\"], verbose=1)\n\n        optimized_kmeans_models[cluster] = model\n\n# Train LSTM per Deep K-Means Cluster\nfor cluster in range(n_clusters):\n    cluster_data = returns[returns['Deep_Cluster'] == cluster].iloc[:, :-2]  # Exclude cluster labels\n    if len(cluster_data) > 1:\n        best_params = optimize_lstm_for_cluster(cluster_data, f\"Deep-{cluster}\")\n        \n        # Prepare Data Again\n        X = np.expand_dims(cluster_data.values[:-1], axis=1)\n        Y = cluster_data.values[1:, 0]\n        X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n        # Train Final Optimized Model\n        model = build_lstm_model((1, cluster_data.shape[1]), best_params[\"lstm_units\"], best_params[\"dropout_rate\"], best_params[\"learning_rate\"])\n        model.fit(X_train, Y_train, epochs=best_params[\"epochs\"], batch_size=best_params[\"batch_size\"], verbose=1)\n\n        optimized_deep_kmeans_models[cluster] = model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}